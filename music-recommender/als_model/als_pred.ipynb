{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Init PySpark session\n",
        "\n",
        "from [this Medium article](https://medium.com/@dipan.saha/pyspark-made-easy-day-2-execute-pyspark-on-google-colabs-f3e57da946a#id_token=eyJhbGciOiJSUzI1NiIsImtpZCI6IjY3MTk2NzgzNTFhNWZhZWRjMmU3MDI3NGJiZWE2MmRhMmE4YzRhMTIiLCJ0eXAiOiJKV1QifQ.eyJpc3MiOiJodHRwczovL2FjY291bnRzLmdvb2dsZS5jb20iLCJhenAiOiIyMTYyOTYwMzU4MzQtazFrNnFlMDYwczJ0cDJhMmphbTRsamRjbXMwMHN0dGcuYXBwcy5nb29nbGV1c2VyY29udGVudC5jb20iLCJhdWQiOiIyMTYyOTYwMzU4MzQtazFrNnFlMDYwczJ0cDJhMmphbTRsamRjbXMwMHN0dGcuYXBwcy5nb29nbGV1c2VyY29udGVudC5jb20iLCJzdWIiOiIxMDc1NTg2ODM5MjQ0NDg3MDEyODciLCJlbWFpbCI6ImNhbWVyb24uY2FsZGVyOTlAZ21haWwuY29tIiwiZW1haWxfdmVyaWZpZWQiOnRydWUsIm5iZiI6MTcxNjc2MTc5OSwibmFtZSI6IkNhbWVyb24gQy4iLCJwaWN0dXJlIjoiaHR0cHM6Ly9saDMuZ29vZ2xldXNlcmNvbnRlbnQuY29tL2EvQUNnOG9jSkk0cUNPUHpqdnVjZ2pOSGhvdngyaWQwcU4yc2NjZHAwSmFaa0h1MGdIMEc1YndDWHdnQT1zOTYtYyIsImdpdmVuX25hbWUiOiJDYW1lcm9uIiwiZmFtaWx5X25hbWUiOiJDLiIsImlhdCI6MTcxNjc2MjA5OSwiZXhwIjoxNzE2NzY1Njk5LCJqdGkiOiI1N2IyOGYwOTMyNTZkOWExMTAzNjk1MGFmYzc4Y2RjN2VjNTZjN2QwIn0.c4jbkhrVpjNnAMUv4KQ8q-mTeiDzWfkQMRwiHkgNKldR4LTlv11nPkUC-t0RKptSkfBa85Tt_v9fnDBkiL1PZyMPRBZfu2l4s5Q7zqZ83Jon964GckhSdkMiMfol3wEMTFtgHmzl3yS_9i6mASEKyGgvRSetc-Ke1m_M-YQldeQbuIt_0aAT5w2fZqKvEfGgQNE6BJGFq1i2kTQy4LI956adVloEt0d23t8fNWkfz7jHr-L6yZUsCiLhF1fqIf5W6qu4gNCr1pcs5FoPoxiBb1kNTlJogUvXNvHd-I-uO5mOfVgtao6Qr9tRwIDZ74JSOAvoL984ZUa8cF4dQQJoZg)"
      ],
      "metadata": {
        "id": "xGy8BgBs_2rW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "!apt-get update # Update apt-get repository.\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null # Install Java.\n",
        "!wget -q http://archive.apache.org/dist/spark/spark-3.1.1/spark-3.1.1-bin-hadoop3.2.tgz # Download Apache Sparks.\n",
        "!tar xf spark-3.1.1-bin-hadoop3.2.tgz # Unzip the tgz file.\n",
        "!pip install -q findspark # Install findspark. Adds PySpark to the System path during runtime.\n",
        "\n",
        "# Set environment variables\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.1.1-bin-hadoop3.2\"\n",
        "\n",
        "!ls\n",
        "\n",
        "# Initialize findspark\n",
        "import findspark\n",
        "findspark.init()\n",
        "\n",
        "# Create a PySpark session\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()\n",
        "spark\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 489
        },
        "id": "en-w_NeU_wVd",
        "outputId": "b746c35f-1c9a-454d-ea5e-26864c18277f"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r0% [Working]\r            \rGet:1 http://security.ubuntu.com/ubuntu jammy-security InRelease [110 kB]\n",
            "Get:2 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,626 B]\n",
            "Hit:3 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
            "Hit:4 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Get:5 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [119 kB]\n",
            "Hit:6 https://ppa.launchpadcontent.net/c2d4u.team/c2d4u4.0+/ubuntu jammy InRelease\n",
            "Hit:7 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:8 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:9 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Hit:10 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n",
            "Get:11 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,375 kB]\n",
            "Get:12 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [2,125 kB]\n",
            "Fetched 3,733 kB in 2s (1,534 kB/s)\n",
            "Reading package lists... Done\n",
            "sample_data  spark-3.1.1-bin-hadoop3.2\tspark-3.1.1-bin-hadoop3.2.tgz\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x7916641856c0>"
            ],
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - in-memory</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://5b549ed9bbb2:4040\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.1.1</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local[*]</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>pyspark-shell</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load Data"
      ],
      "metadata": {
        "id": "SlIC7DXjB5LL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from pyspark.ml.evaluation import RegressionEvaluator\n",
        "from pyspark.ml.recommendation import ALS\n",
        "from pyspark.sql.types import IntegerType\n"
      ],
      "metadata": {
        "id": "onP00DrH_k4v"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "trainFile = \"/content/drive/MyDrive/Notebooks/data/trainItem.data\"\n",
        "testFile = \"/content/drive/MyDrive/Notebooks/data/testItem.data\"\n"
      ],
      "metadata": {
        "id": "dQ07L6ITB2pu"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fbDDuF6A_cgs",
        "outputId": "4168197b-488b-45ed-caf8-0022d48a88cb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+------+------+\n",
            "|userID|itemID|rating|\n",
            "+------+------+------+\n",
            "|199808|248969|  90.0|\n",
            "|199808|  2663|  90.0|\n",
            "|199808| 28341|  90.0|\n",
            "|199808| 42563|  90.0|\n",
            "|199808| 59092|  90.0|\n",
            "+------+------+------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# load training data file into pySpark dataFrame format\n",
        "training = spark.read.csv(trainFile, header = False)\n",
        "training = training.withColumnRenamed(\"_c0\", \"userID\").withColumnRenamed(\"_c1\", \"itemID\").withColumnRenamed(\"_c2\", \"rating\")\n",
        "# assign training dataFrame column data types (dataFrame by default herein assume 'string' type)\n",
        "training = training.withColumn(\"userID\", training[\"userID\"].cast(IntegerType()))\n",
        "training = training.withColumn(\"itemID\", training[\"itemID\"].cast(IntegerType()))\n",
        "training = training.withColumn(\"rating\", training[\"rating\"].cast('float'))\n",
        "\n",
        "training.show(5)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# load testing data file into pySpark dataFrame format\n",
        "testing = spark.read.csv(testFile, header = False)\n",
        "testing = testing.withColumnRenamed(\"_c0\", \"userID\").withColumnRenamed(\"_c1\", \"itemID\").withColumnRenamed(\"_c2\", \"rating\")\n",
        "testing = testing.withColumn(\"userID\", testing[\"userID\"].cast(IntegerType()))\n",
        "testing = testing.withColumn(\"itemID\", testing[\"itemID\"].cast(IntegerType()))\n",
        "testing = testing.withColumn(\"rating\", testing[\"rating\"].cast('float'))\n",
        "\n",
        "testing.show(5)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S8GgyIJLCrO4",
        "outputId": "d14a2203-5ae9-4507-c128-e364f4e1e693"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+------+------+\n",
            "|userID|itemID|rating|\n",
            "+------+------+------+\n",
            "|199810|208019|   0.0|\n",
            "|199810| 74139|   0.0|\n",
            "|199810|  9903|   0.0|\n",
            "|199810|242681|   0.0|\n",
            "|199810| 18515|   0.0|\n",
            "+------+------+------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Create ALS model\n",
        "als = ALS(\n",
        "    maxIter=5,\n",
        "    rank = 5,\n",
        "    regParam=0.01,\n",
        "    userCol=\"userID\",\n",
        "    itemCol=\"itemID\",\n",
        "    ratingCol=\"rating\",\n",
        "    nonnegative = True,\n",
        "    implicitPrefs = False,\n",
        "    coldStartStrategy=\"drop\"\n",
        ")\n",
        "# fit the ALS model using the training set\n",
        "model = als.fit(training)\n"
      ],
      "metadata": {
        "id": "I94m6NzwClAW"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# predict ratings with testing\n",
        "predictions = model.transform(testing)\n",
        "predictions.show(5)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gWEI0RgyDWC_",
        "outputId": "c91480c7-b235-48bf-c4d9-30473462da62"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+------+------+----------+\n",
            "|userID|itemID|rating|prediction|\n",
            "+------+------+------+----------+\n",
            "|230073|   463|   0.0| 123.03484|\n",
            "|230962|   471|   0.0|  86.49538|\n",
            "|218845|  1088|   0.0| 153.73868|\n",
            "|209697|  1088|   0.0|  46.83917|\n",
            "|224445|  2142|   0.0| 30.431276|\n",
            "+------+------+------+----------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# save dataframe into a folder 'predictions' with a single file (coalesce(1))\n",
        "# note: cannot assign the filename\n",
        "predictions.coalesce(1).write.csv(\"predictions\")\n",
        "\n",
        "# save predictions result to a file\n",
        "predictions.toPandas().to_csv('myprediction.csv')\n"
      ],
      "metadata": {
        "id": "2-eLBvUeCnET"
      },
      "execution_count": 20,
      "outputs": []
    }
  ]
}